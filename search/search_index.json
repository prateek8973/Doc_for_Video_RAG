{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Voice Activated Video Search using RAG","text":"<p>Welcome to the official documentation!</p> <ul> <li> <p>This page contains the documentation for Voice Activated Video Search using RAG</p> </li> <li> <p>To help you understand and use the app effectively, the documentation is divided into the following sections:</p> </li> <li> <p>Tech Stack \u2014 All the technologies used in this app.</p> </li> <li>Whisper Statistics- Whisper model comparison and statistics.</li> <li>Architecture \u2014 Component-level architecture diagram and explanation.</li> <li>Process Flow \u2014 End-to-end flow from video upload to response.</li> <li>How to Use \u2014 Instructions to set up, run, and interact with the app.</li> <li>Different containers created using docker -</li> </ul> <p>\u26a0\ufe0f Before you start: To use the application, please make sure you have a valid API key from either: - Google AI Studio (for Gemini) - Cerebras API Console (for Cerebras LLM) </p> <p>You'll need to enter this key when interacting with the app to enable LLM-based responses.</p>"},{"location":"#what-this-app-does","title":"What This App Does","text":"<ul> <li>Converts speech in videos to text using Whisper</li> <li>Generates semantic embeddings with MiniLM for deep search</li> <li>Uses Gemini and Cerebras(Llama models) models LLM-based responses</li> <li>Supports multiple video files with timestamped insights</li> <li>Uses FAISS to index and search over large transcript databases</li> <li>Allows users to ask questions and get audio + text responses.</li> <li>Uses the Llamaindex framework to index and search over large transcript databases.</li> </ul>"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#architecture","title":"Architecture","text":"<ul> <li> <p>Vector DB operations. </p> </li> <li> <p>Metadata handling using Llamaindex. </p> </li> <li> <p>API Endpoints. </p> </li> </ul>"},{"location":"dockerisations/","title":"Dockerisation","text":""},{"location":"dockerisations/#dockerisation-process","title":"Dockerisation process","text":""},{"location":"process-flow/","title":"Process Flow","text":""},{"location":"process-flow/#process-flow","title":"Process Flow","text":"<ul> <li>Complete process flow. </li> </ul>"},{"location":"tech-stack/","title":"\u2699\ufe0f Tech Stack","text":"<p>This section outlines the core technologies used in building the Voice-Based Video Search App, categorized by function.</p>"},{"location":"tech-stack/#frontend","title":"\ud83d\udda5\ufe0f Frontend","text":"Technology Purpose React Frontend framework for building UI Vite Lightning-fast dev/build tool JavaScript Programming language for client logic Tailwind CSS (optional) Utility-first CSS framework for design"},{"location":"tech-stack/#backend","title":"\ud83e\udde0 Backend","text":"Technology Purpose FastAPI REST API backend framework (Python) Uvicorn ASGI server for running FastAPI moviepy Extracts audio from uploaded videos whisper Transcribes video audio to text"},{"location":"tech-stack/#embeddings-vector-search","title":"\ud83e\uddee Embeddings &amp; Vector Search","text":"Technology Purpose Hugging Face Transformers Provides <code>all-MiniLM-L6-v2</code> embedding model FAISS High-speed vector similarity search"},{"location":"tech-stack/#llm-rag-retrieval-augmented-generation","title":"\ud83e\udde0 LLM &amp; RAG (Retrieval-Augmented Generation)","text":"Technology Purpose Gemini 1.5 flash LLM for inference Gemini 2.5 flash LLM for inference Gemini 2.0 flash LLM for inference Llama 3.1 8b LLM for inference Llama 70b LLM for inference"},{"location":"tech-stack/#audio-response-features","title":"\ud83c\udfa7 Audio &amp; Response Features","text":"Technology Purpose gTTS / pyttsx3 Converts response text to speech"},{"location":"tech-stack/#deployment","title":"\ud83d\udee0\ufe0f Deployment","text":"Platform Purpose Docker"},{"location":"tech-stack/#summary","title":"\u2705 Summary","text":"<p>This app is built using free, open-source, and local-first tools to ensure accessibility without reliance on paid APIs or GPUs. The stack is modular and can be extended or optimized as needed.</p>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#usage","title":"Usage","text":"<ul> <li>There are two components<ul> <li>Backend(FastAPI with Llamaindex)</li> <li>Frontend (Vite+React)</li> </ul> </li> </ul> <p>This guide helps you set up and run the Voice-Based Video Search App on your machine.</p>"},{"location":"usage/#prerequisites","title":"\ud83e\uddf1 Prerequisites","text":"<p>Ensure you have:</p> <ul> <li>Python 3.10 or newer</li> <li><code>ffmpeg</code> installed (used by <code>moviepy</code>)</li> <li>A valid API key from Google AI Studio or Cerebras</li> </ul>"},{"location":"usage/#install-dependencies","title":"\ud83d\udce6 Install Dependencies","text":"<p>Create a virtual environment (recommended):</p> <pre><code>python -m venv venv\nsource venv/bin/activate  # or venv\\Scripts\\activate on Windows\n</code></pre> <ul> <li><code>pip install -r requirments.txt</code></li> </ul> <pre><code># Core dependencies\nfastapi\nuvicorn\npydantic\npython-multipart\ntyping-extensions\n\n\nmoviepy\nfaster-whisper\n\n\nllama-index\nllama-index-embeddings-huggingface\nllama-index-llms-openai\nllama-index-llms-cohere\nllama-index-llms-huggingface\nsentence-transformers\nfaiss-cpu\ntorch\ntransformers\n\n\n\n# Data processing\nnumpy\nscikit-learn\nmatplotlib\npandas\n</code></pre>"},{"location":"whisper/","title":"Whisper Stats","text":""},{"location":"whisper/#whisper-processing-and-statistics","title":"Whisper Processing and Statistics","text":"<ul> <li> <p>Whisper's small and medium models hit the right balance for most transcription work. </p> </li> <li> <p>They give good results without being too slow or using too much computer power</p> </li> <li> <p>Whisper_s2t offers faster inference and better memory management compared to the original Whisper library. </p> </li> <li> <p>It uses CTranslate2 backend to optimize performance, making it more practical for real-world speech-to-text applications.</p> </li> <li> <p>Whisper model Transcription Speed </p> </li> <li> <p>Whisper Model Transcription Time </p> </li> </ul>"}]}